# 生成AI技術詳細 - Q&A資料

プレゼンテーション補足資料（質疑応答用）

---

## 1. Transformerアーキテクチャ

### 基本概念

**Transformer（2017年Google発表）**
- 従来のRNN/LSTMに代わる新しいニューラルネットワーク構造
- "Attention is All You Need"論文で発表
- 現在のすべての生成AI（GPT、Claude、Gemini等）の基盤技術

### 従来技術との違い

**従来のAI（RNN/LSTM）:**
```
処理方法: 順次処理
「猫」→「が」→「魚」→「を」→「食べた」

問題点:
- 長い文章では最初の情報を忘れる
- 並列処理不可（遅い）
- 長距離依存関係を捉えにくい
```

**Transformer:**
```
処理方法: 並列処理
すべての単語を同時に処理し、相互関係を計算

利点:
- 長い文章でも文脈保持
- GPU/TPUで並列処理可能（速い）
- 長距離依存関係を直接捉える
```

### Self-Attention機構（核心技術）

**仕組み:**

例文: 「猫が魚を食べた」

1. **各単語のベクトル化**
   ```
   猫: [0.2, 0.8, 0.3, 0.1, ...]（512次元など）
   が: [0.1, 0.3, 0.5, 0.2, ...]
   魚: [0.7, 0.2, 0.4, 0.3, ...]
   を: [0.3, 0.1, 0.6, 0.4, ...]
   食べた: [0.5, 0.9, 0.2, 0.7, ...]
   ```

2. **3つの変換（Q/K/V）**
   各単語から3種類のベクトルを生成:
   - **Query（質問）**: 「私は誰に注目すべき？」
   - **Key（鍵）**: 「私はこういう特徴です」
   - **Value（値）**: 「私の実際の意味情報」

3. **Attention計算**
   ```
   「猫」のQueryと、全単語のKeyの類似度:

   猫 · 猫 = 0.9  （自分自身: 強い関係）
   猫 · が = 0.1  （助詞: 弱い関係）
   猫 · 魚 = 0.7  （目的語: やや強い関係）
   猫 · を = 0.1  （助詞: 弱い関係）
   猫 · 食べた = 0.8  （動詞: 強い関係）
   ```

4. **Softmax（確率化）**
   ```
   猫: 35%
   が: 3%
   魚: 25%
   を: 3%
   食べた: 34%
   ```

5. **重み付け合計**
   ```
   猫の新しい表現 =
     0.35×猫Value +
     0.03×がValue +
     0.25×魚Value +
     0.03×をValue +
     0.34×食べたValue
   ```

**数式:**
```
Attention(Q, K, V) = softmax(Q·K^T / √d) · V

Q: Query行列
K: Key行列
V: Value行列
d: ベクトルの次元数（スケーリング用）
```

### Multi-Head Attention

実際には8〜96個の異なるAttentionを並列実行:

```
Head 1: 主語-述語関係に注目
Head 2: 修飾語-被修飾語関係
Head 3: 時制・文法関係
Head 4: 意味的類似性
...
```

それぞれが異なる「視点」で文章を理解し、最後に統合。

### なぜ大規模化に向いているか

1. **並列処理可能**
   - すべての計算が行列演算
   - GPU/TPUで超高速処理

2. **長距離依存関係**
   - 文章の最初と最後を直接結びつける

3. **スケーラブル**
   - レイヤーを増やせば深く理解
   - パラメータを増やせば複雑なパターンを学習

---

## 2. 「ランダム生成」の正体

### 文章生成プロセス

**ステップバイステップ:**

入力: 「人工知能の未来は」

1. **文脈理解（Transformer処理）**
   ```
   Attention機構で全体の文脈を理解
   → 次に来るべき単語の特徴を計算
   ```

2. **確率分布の計算**
   ```
   次の単語候補とその確率:

   明るい: 30%
   期待される: 25%
   不透明: 20%
   重要: 15%
   課題が多い: 8%
   その他: 2%
   ```

3. **サンプリング（ランダム選択）**
   ```
   確率に応じてランダムに1つ選択

   実行1: 「明るい」（30%の確率で当選）
   実行2: 「期待される」（25%の確率で当選）
   実行3: 「明るい」（また30%で当選）
   ```

4. **次のステップ**
   ```
   新しい入力: 「人工知能の未来は明るい」
   → また次の単語の確率計算...

   このループを繰り返す
   ```

### Temperature パラメータ

**Temperature = 0（決定的）**
```
確率分布の変化:
明るい: 100%  ← 常にこれを選ぶ
期待される: 0%
不透明: 0%
...

結果: 毎回全く同じ文章
用途: 事実的回答、再現性が必要な場合
```

**Temperature = 0.7（バランス型・デフォルト）**
```
確率分布をやや尖らせる:
明るい: 42%
期待される: 28%
不透明: 18%
重要: 9%
課題が多い: 3%

結果: 自然で多少の変化あり
用途: 一般的な対話、バランスの良い回答
```

**Temperature = 1.0（元の確率）**
```
計算された確率そのまま:
明るい: 30%
期待される: 25%
不透明: 20%
重要: 15%
課題が多い: 8%
その他: 2%

結果: やや多様性が増す
```

**Temperature = 1.5（創造的）**
```
確率分布を平らにする:
明るい: 24%
期待される: 22%
不透明: 20%
重要: 18%
課題が多い: 12%
その他: 4%

結果: 非常に創造的だが不正確になりがち
用途: ブレインストーミング、創作
```

### Top-p（Nucleus Sampling）

Temperatureと組み合わせて使う別の制御方法:

```
Top-p = 0.9の場合:

確率の高い順に並べ:
明るい: 30%（累積30%）
期待される: 25%（累積55%）
不透明: 20%（累積75%）
重要: 15%（累積90%）← ここまで
課題が多い: 8%（累積98%）← 除外
その他: 2%（累積100%）← 除外

累積確率90%までの候補だけを使う
```

### なぜランダム性が必要か

**ランダム性なし（Temperature=0）:**
```
利点:
- 予測可能
- 再現性がある
- 事実的

欠点:
- 同じ質問に毎回同じ答え
- 創造性ゼロ
- 単調
```

**ランダム性あり（Temperature>0）:**
```
利点:
- 表現が多様
- 創造的
- 自然な対話

欠点:
- 毎回少し違う
- 時に不正確
```

### 実際のAPI設定例

ChatGPT API:
```json
{
  "model": "gpt-4",
  "temperature": 0.7,
  "top_p": 0.9,
  "messages": [...]
}
```

Claude API:
```json
{
  "model": "claude-sonnet-4.5",
  "temperature": 1.0,
  "max_tokens": 4096,
  "messages": [...]
}
```

---

## 3. よくある質問

### Q1: AIは本当に「理解」しているのか？

**技術的回答:**
- Attention機構で文脈の関係性を計算
- 確率的に次の単語を予測
- 内部表現（ベクトル）で意味を保持

**哲学的問題:**
- 「理解」の定義次第
- 人間と同じメカニズムではない
- しかし結果は区別困難

**実用的回答:**
- 実用上は「理解している」と見なせる
- タスク遂行能力で判断すべき

### Q2: なぜ時々間違えるのか？

**原因:**
1. **確率的サンプリング**
   - 低確率の選択肢を選ぶこともある

2. **学習データの偏り**
   - 学習データにない知識は答えられない
   - 間違った情報を学習している可能性

3. **幻覚（Hallucination）**
   - 確率的にもっともらしい嘘を生成
   - 特にTemperature高い時

**対策:**
- Temperature下げる（0.3〜0.5）
- 重要な情報は検証する
- RAG（外部知識ベース）を使う

### Q3: パラメータ数が多いほど賢いのか？

**一般的傾向:**
- Yes、大規模ほど高性能

**ただし:**
- 学習データ量も重要
- 学習方法（RLHF等）も重要
- タスクによって最適サイズが異なる

**例外:**
- DeepSeek R1: 小規模だが推論特化で高性能
- Phi-4: 14Bパラメータで大規模モデル並み

### Q4: コンテクスト長が長いと何が良いのか？

**利点:**
1. **長文理解**
   - 論文、契約書、マニュアル全文を一度に処理

2. **対話履歴保持**
   - 長い会話の文脈を保つ

3. **複雑なタスク**
   - 複数文書の比較
   - 大量のコードベース理解

**欠点:**
- コストが高い（トークン課金）
- 処理が遅くなる
- 必要ない場合は短い方が良い

### Q5: オープンソースモデルとクローズドモデルの違い

**クローズドモデル（GPT-4、Claude等）:**
```
利点:
- 最高性能
- 継続的改善
- サポートあり

欠点:
- コスト高い
- カスタマイズ不可
- データプライバシー懸念
```

**オープンソース（Llama、Qwen等）:**
```
利点:
- 無料使用可能
- カスタマイズ自由
- プライベート運用可能

欠点:
- 自前インフラ必要
- 性能はやや劣る（差は縮小中）
- 自己責任
```

---

## 4. 技術トレンド（2025年）

### 推論特化モデル

**o-series（OpenAI）、R1（DeepSeek）:**
- 通常のLLMと異なる学習方法
- 内部で「考える」プロセスを持つ
- 数学、論理、コーディングで大幅性能向上

**仕組み（推定）:**
```
従来: プロンプト → 即座に回答

推論モデル: プロンプト →
  内部思考（数千トークン） →
  検証・修正 →
  最終回答
```

### マルチモーダル化

- テキスト + 画像 + 音声 + 動画
- リアルタイム音声対話（GPT-4o）
- 統合理解（動画の内容を見て説明）

### エージェント化

- 自律的タスク実行
- 外部ツール使用（ブラウザ、API等）
- MCP（Model Context Protocol）

---

## 5. 参考文献・リソース

### 論文
- "Attention is All You Need" (Vaswani et al., 2017)
- "Language Models are Few-Shot Learners" (GPT-3, Brown et al., 2020)
- "Training language models to follow instructions with human feedback" (InstructGPT, Ouyang et al., 2022)

### 技術ブログ
- OpenAI Research Blog
- Anthropic Research
- Google AI Blog

### 学習リソース
- Andrej Karpathy: Neural Networks: Zero to Hero（YouTube）
- The Illustrated Transformer（Jay Alammar）
- FastAI Course

---

*作成日: 2025-10-04*
*更新予定: プレゼン後の質問を反映*
